{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Building Trec collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the tokenizer with its different capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def std_tokenizer_build_standard_vocabulary(queries,documents,min_occ = 2,limit_docs = None, limit_queries = None):\n",
    "    \"\"\"Function that builds the standard vocabulary from a list of queries and a list of documents and\n",
    "    with a limit on the number of documents and queries to manipulate\"\"\"\n",
    "    vocabulary = Counter()\n",
    "    \n",
    "    count = 0\n",
    "    for _,document in documents.iterrows():\n",
    "        for word in document[0].split(\" \"):\n",
    "            vocabulary[word] += 1\n",
    "        count +=1\n",
    "        if count==limit_docs:\n",
    "            break\n",
    "            \n",
    "    count = 0\n",
    "    for _,query in queries.iterrows():\n",
    "        for word in query[0].split(\" \"):\n",
    "            vocabulary[word] += 1\n",
    "        count +=1\n",
    "        if count==limit_queries:\n",
    "            break\n",
    "    \n",
    "    vocabulary = {i:elem[0] for i,elem in enumerate(vocabulary.most_common()) if elem[1] >= min_occ}\n",
    "    \n",
    "    for key in list(vocabulary):\n",
    "        vocabulary[vocabulary[key]] = key\n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "def std_tokenizer_index(pdDataFrame,vocabulary,stemmer=None):\n",
    "    \"\"\"Function that indexes a dataframe either documents or queries for example according to a vocabulary.\n",
    "    While doing that it can perform a stemmerization if the vocabulary was built on words that got stemmerized\"\"\"\n",
    "    indexed_elements = []\n",
    "    index = dict()\n",
    "    count = 0\n",
    "    if stemmer is None:\n",
    "        for key,element in pdDataFrame.iterrows():\n",
    "            indexed_elements.append([vocabulary[elem.lower()] for elem in element[0].split(\" \") if elem.lower() in vocabulary])\n",
    "            index[str(key)] = count\n",
    "            index[count] = str(key)\n",
    "            count += 1\n",
    "            \n",
    "    else:\n",
    "        for key,element in pdDataFrame.iterrows():\n",
    "            indexed_elements.append([vocabulary[stemmer.stem(elem.lower())] for elem in element[0].split(\" \") if stemmer.stem(elem.lower()) in vocabulary])\n",
    "            index[str(key)] = count\n",
    "            index[count] = str(key)\n",
    "            count += 1\n",
    "\n",
    "    return index,indexed_elements\n",
    "\n",
    "\n",
    "\n",
    "def std_tokenizer_index_dict(pdDataFrame,vocabulary):\n",
    "    \"\"\"Function that indexes a dict that could be documents or queries for example according to a vocabulary\"\"\"\n",
    "    indexed_elements = []\n",
    "    index = dict()\n",
    "    count = 0\n",
    "    for key,element in pdDataFrame.items():\n",
    "        indexed_elements.append([vocabulary[elem] for elem in element.split(\" \") if elem in vocabulary])\n",
    "        index[str(key)] = count\n",
    "        index[count] = str(key)\n",
    "        count += 1\n",
    "    return index,indexed_elements\n",
    "\n",
    "\n",
    "def std_tokenizer_preprocess(queries,documents,min_occ = 5):\n",
    "    \"\"\"Function that preprocesses queries and documents. It builds the standard vocabulary and indexes both\n",
    "    the documents and the queries and returns the vocabulary , the query and doc index and the indexed elements of \n",
    "    both doc and  query\"\"\"\n",
    "    vocabulary = std_tokenizer_build_standard_vocabulary(queries,\n",
    "                                           documents,\n",
    "                                           min_occ = min_occ)\n",
    "    \n",
    "    doc_index,indexed_docs = std_tokenizer_index(documents,vocabulary)\n",
    "    \n",
    "    query_index,indexed_queries = std_tokenizer_index(queries,vocabulary)\n",
    "    \n",
    "    return vocabulary,query_index,indexed_queries,doc_index,indexed_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading read_qrels and read_trec_train_qrels from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pytrec_eval\n",
    "from collections import Counter\n",
    "\n",
    "def read_qrels(path):\n",
    "    qrels = []\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            rel = line.split()\n",
    "            qrels.append([rel[0],rel[2]])\n",
    "    return qrels\n",
    "\n",
    "def read_trec_train_qrels(path):\n",
    "    pos_qrels = []\n",
    "    neg_qrels = dict()\n",
    "    with open(path,'r') as f:\n",
    "        for line in f:\n",
    "            rel = line.split()\n",
    "            if rel[3] == '1':\n",
    "                pos_qrels.append([rel[0],rel[2]])\n",
    "            else :\n",
    "                if rel[0] not in neg_qrels:\n",
    "                    neg_qrels[rel[0]] = [rel[2]]\n",
    "                else:\n",
    "                    neg_qrels[rel[0]].append(rel[2])\n",
    "    return {'pos':pos_qrels,'neg':neg_qrels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Trec Collections class and its methods\n",
    "It contains the definition of a class called TrecCollection which has various useful methods for processing the Trec collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pytrec_eval\n",
    "import pandas as pd\n",
    "from nltk.stem import snowball\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def build_folds(queries_ids,k=5):\n",
    "    \"\"\"Builds folds for the K-fold cross validation\"\"\"\n",
    "    nb_queries = len(queries_ids)\n",
    "    nb_elem = int(nb_queries/k)\n",
    "    random.shuffle(queries_ids)\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        folds.append(queries_ids[i*nb_elem:(i+1)*nb_elem])\n",
    "    return folds\n",
    "\n",
    "\n",
    "\n",
    "def read_queries(queries_path):\n",
    "    \"\"\" Function that reads the queries from a path of the file containing those queries. It returns  dict of \n",
    "    query ids and query texts\"\"\"\n",
    "    queries_ids = []\n",
    "    queries_text = []\n",
    "    with open(queries_path,'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"<num>\"):\n",
    "                queries_ids.append(int(line[line.find(':')+1:-1]))\n",
    "            if line.startswith(\"<title>\"):\n",
    "                pos = max(line.find(':'),line.find('>'))\n",
    "                queries_text.append(\"\".join([char for char in line[pos+2:-1] if char not in string.punctuation]))\n",
    "\n",
    "    return dict(zip(queries_ids, queries_text))\n",
    "\n",
    "\n",
    "def read_documents(documents_path):\n",
    "    \"\"\"Same function but for documents\"\"\"\n",
    "    doc_ids = []\n",
    "    doc_text = ['']\n",
    "    fill_text = False\n",
    "    with open(documents_path,'r',encoding='latin1') as f:\n",
    "\n",
    "        for i,line in enumerate(f):\n",
    "            if \"<DOCNO>\" in line : \n",
    "                doc_ids.append(line.strip(\"<DOCNO> \").strip(\" </DOCNO>\\n\"))\n",
    "            if \"<TEXT>\" in line:\n",
    "                fill_text = True\n",
    "                continue\n",
    "            if \"</TEXT>\" in line:\n",
    "                continue\n",
    "            if \"</DOC>\" in line:\n",
    "                doc_text.append('')\n",
    "                fill_text = False\n",
    "            elif fill_text:\n",
    "                doc_text[-1] += line\n",
    "    del doc_text[-1]\n",
    "\n",
    "    for i in range(len(doc_text)):\n",
    "        doc_text[i] = \" \".join(doc_text[i].replace('\\n',' ').split())\n",
    "        doc_text[i] = \"\".join(char for char in doc_text[i] if char not in string.punctuation)\n",
    "\n",
    "    return dict(zip(doc_ids, doc_text))\n",
    "\n",
    "\n",
    "def save_qrel(path,qrels,subset):\n",
    "    \"\"\"Function that saves query document relavence scores into a file.\"\"\"\n",
    "    with open(path,'w') as f:\n",
    "        for query in subset:\n",
    "            for doc,rel in qrels[str(query)].items():\n",
    "                f.write(str(query) + '\\t0\\t' + doc + '\\t' + str(rel) + '\\n')\n",
    "                \n",
    "                \n",
    "def save_queries_csv(coll_path,queries,folds):\n",
    "    \"\"\"Function that saves folds of queries into csv files for each fold\"\"\"\n",
    "    for i,elem in enumerate(folds):\n",
    "        index = pd.Index([key for key in elem],name = 'id_left')\n",
    "        d = {\"text_left\":[queries[key] for key in elem]}\n",
    "        pd.DataFrame(data=d,index=index).to_csv(coll_path + '/fold'  + str(i) + '/queries.csv')\n",
    "        \n",
    "    \n",
    "def save_documents_csv(coll_path,documents):\n",
    "    \"\"\"Function that saves documents into a csv file\"\"\"\n",
    "    index = pd.Index([key for key in documents],name = 'id_right')\n",
    "    d = {\"text_right\":[documents[key] for key in documents]}\n",
    "    pd.DataFrame(data=d,index=index).to_csv(coll_path + '/documents.csv')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def read_collection(collection_paths = ['TREC/AP88-89','TREC/LA','TREC/FT91-94'],k=5):\n",
    "    \"\"\"Function that for every collection reads queries , create folds for the Kfold cross validation\n",
    "    ,reads the collection qrels ,save qrels and queries for each fold,reads documents\n",
    "    on xml format and saves them into csv format\"\"\"\n",
    "    for collection_path in collection_paths:\n",
    "\n",
    "        queries = read_queries(collection_path + '/queries')\n",
    "\n",
    "        folds = build_folds(list(queries.keys()),k=k)\n",
    "\n",
    "        with open(collection_path + '/qrels', 'r') as f_qrel:\n",
    "            qrel = pytrec_eval.parse_qrel(f_qrel)\n",
    "\n",
    "        for i,fold in enumerate(folds):\n",
    "            if not os.path.exists(collection_path + '/fold'  + str(i)):\n",
    "                os.makedirs(collection_path + '/fold'  + str(i))    \n",
    "            save_qrel(collection_path + '/fold'+ str(i) + '/qrels',qrel,fold)\n",
    "\n",
    "\n",
    "        save_queries_csv(collection_path,queries,folds)\n",
    "\n",
    "        documents = read_documents(collection_path + '/documents.xml')\n",
    "\n",
    "        save_documents_csv(collection_path,documents)\n",
    "    \n",
    "class TrecCollection:\n",
    "    def __init__(self,k=5,language='english'):\n",
    "        self.documents = None\n",
    "        self.k = k\n",
    "        self.language = language\n",
    "        self.stemmer = snowball.EnglishStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        \n",
    "    def load_collection(self,collection_path):\n",
    "        \"\"\"Function that loads the collection : it loads documents and the folds containing the queries per fold\n",
    "        in Csv format,qrels per fold and the training qrels per fold . It is run after the function read_collection\"\"\"\n",
    "        self.documents = pd.read_csv(collection_path + '/documents.csv',index_col='id_right',na_filter=False)\n",
    "        \n",
    "        self.folds_queries = []\n",
    "        self.folds_qrels = []\n",
    "        self.folds_training_qrels = []\n",
    "        for i in range(self.k):\n",
    "            self.folds_queries.append(pd.read_csv(collection_path + '/fold' + str(i) + '/queries.csv',\n",
    "                                                  index_col='id_left',\n",
    "                                                  na_filter=False))\n",
    "            self.folds_qrels.append(read_qrels(collection_path + '/fold' + str(i) + '/qrels'))\n",
    "            self.folds_training_qrels.append(read_trec_train_qrels(collection_path + '/fold' + str(i) + '/qrels'))\n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "    def update_standard_vocabulary(self,sequences,remove_stopwords=True):\n",
    "        \"\"\"Function that updates the standard vocabulary using new sequences\"\"\"\n",
    "        count = 0\n",
    "        if remove_stopwords:\n",
    "            for _,sequence in sequences.iterrows():\n",
    "                for word in sequence[0].split(\" \"):\n",
    "                    temp = word.lower()\n",
    "                    if temp not in self.stop_words:\n",
    "                        self.vocabulary[self.stemmer.stem(temp)] += 1\n",
    "                count +=1\n",
    "        else:\n",
    "            for _,sequence in sequences.iterrows():\n",
    "                for word in sequence[0].split(\" \"):\n",
    "                    self.vocabulary[self.stemmer.stem(word.lower())] += 1\n",
    "                count +=1\n",
    "        \n",
    "        \n",
    "    def build_standard_vocabulary(self,\n",
    "                                  min_occ = 2,\n",
    "                                  remove_stopwords = True):\n",
    "        \"\"\"Function that builds the standard vocabulary from documents with minimum occurence equal to 2\"\"\"\n",
    "        self.vocabulary = Counter()\n",
    "    \n",
    "        self.update_standard_vocabulary(self.documents,remove_stopwords)\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            self.update_standard_vocabulary(self.folds_queries[i],remove_stopwords)\n",
    "                \n",
    "        del self.vocabulary['']\n",
    "                \n",
    "        self.vocabulary = {i+1:elem[0] for i,elem in enumerate(self.vocabulary.most_common()) if elem[1] >= min_occ}\n",
    "\n",
    "        for key in list(self.vocabulary):\n",
    "            self.vocabulary[self.vocabulary[key]] = key\n",
    "            \n",
    "        self.vocabulary[0] = '<PAD>'\n",
    "        self.vocabulary['<PAD>'] = 0\n",
    "    \n",
    "     \n",
    "        \n",
    "    def standard_preprocess(self,\n",
    "                            remove_stopwords = True,\n",
    "                            min_occ = 5):\n",
    "        \"\"\"General function that preprocesses the Trec collection by building vocabulary, using the tokenizer\n",
    "        to index documents, the folds of queries and all the queries. It is run after the method \n",
    "        load_collection\"\"\"\n",
    "        self.build_standard_vocabulary(min_occ = min_occ,\n",
    "                                       remove_stopwords = remove_stopwords)\n",
    "                \n",
    "        self.doc_index,self.indexed_docs = std_tokenizer_index(self.documents,\n",
    "                                                               self.vocabulary,\n",
    "                                                               self.stemmer)\n",
    "        \n",
    "        self.queries_index = []\n",
    "        self.indexed_queries = []\n",
    "        \n",
    "        for i in range(self.k):\n",
    "        \n",
    "            queries_index,indexed_queries = std_tokenizer_index(self.folds_queries[i],\n",
    "                                                                self.vocabulary,\n",
    "                                                                self.stemmer)\n",
    "            self.queries_index.append(queries_index)\n",
    "            self.indexed_queries.append(indexed_queries)\n",
    "            \n",
    "        \n",
    "        self.all_indexed_queries = []\n",
    "        for elem in self.indexed_queries:\n",
    "            self.all_indexed_queries+=elem\n",
    "        \n",
    "        self.all_queries_index = dict()\n",
    "        counter = 0\n",
    "        for i in range(len(self.queries_index)):\n",
    "            for j in range(int(len(self.queries_index[i])/2)):\n",
    "                self.all_queries_index[counter] = self.queries_index[i][j]\n",
    "                self.all_queries_index[self.queries_index[i][j]] = counter\n",
    "                counter +=1\n",
    "    \n",
    "    \n",
    "    def build_inverted_index(self):\n",
    "        \"\"\"Function that builds the inverted index of documents \"\"\"\n",
    "        self.inverted_index = dict()\n",
    "        \n",
    "        for token in self.vocabulary:\n",
    "            if isinstance(token, int):\n",
    "                self.inverted_index[token] = Counter()\n",
    "            \n",
    "        for i,indexed_document in enumerate(self.indexed_docs):\n",
    "            for token in indexed_document:\n",
    "                self.inverted_index[token][i] += np.float32(1.0)\n",
    "                \n",
    "                \n",
    "    def compute_idf(self):\n",
    "        \"\"\"Funciton that computes the idf of every term in te inverted index\"\"\"\n",
    "        nb_docs = len(self.doc_index)\n",
    "        self.idf = {token:np.log((nb_docs+1)/(1+len(self.inverted_index[token]))) for token in self.inverted_index }\n",
    "        \n",
    "        \n",
    "    def compute_docs_length(self):\n",
    "        \"\"\"Function that computes the length of each document in the collection\"\"\"\n",
    "        self.docs_length = {i:len(doc) for i,doc in enumerate(self.indexed_docs)}\n",
    "        \n",
    "        \n",
    "    def compute_collection_frequencies(self):\n",
    "        \"\"\"Function that computes frequency of words in the collection\"\"\"\n",
    "        coll_length = sum([value for key,value in self.docs_length.items()])\n",
    "        self.c_freq = {token:sum([freq for _,freq in self.inverted_index[token].items()])/coll_length for token in self.inverted_index}\n",
    "        \n",
    "    def index_relations(self):\n",
    "#         self.folds_queries = []\n",
    "#         self.folds_qrels = []\n",
    "#         self.folds_training_qrels = []\n",
    "        \n",
    "        self.folds_indexed_qrels = []\n",
    "        self.folds_training_indexed_qrels = []\n",
    "        \n",
    "        for i in range(self.k):\n",
    "        \n",
    "            training_indexed_qrels = dict()\n",
    "            training_indexed_qrels['pos'] = []\n",
    "            training_indexed_qrels['neg'] = dict()\n",
    "            for elem in self.folds_training_qrels[i]['pos']:\n",
    "                if elem[1] in self.doc_index:\n",
    "                    training_indexed_qrels['pos'].append([self.all_queries_index[elem[0]],\n",
    "                                                          self.doc_index[elem[1]]])\n",
    "\n",
    "            for key in self.folds_training_qrels[i]['neg']:\n",
    "                training_indexed_qrels['neg'][key] = []\n",
    "                for elem in self.folds_training_qrels[i]['neg'][key]:\n",
    "                    if elem in self.doc_index:\n",
    "                        training_indexed_qrels['neg'][key].append(self.doc_index[elem])\n",
    "\n",
    "            self.folds_training_indexed_qrels.append(training_indexed_qrels)\n",
    "\n",
    "            indexed_qrels = []\n",
    "            for elem in self.folds_qrels[i]:\n",
    "                if elem[1] in self.doc_index:\n",
    "                    indexed_qrels.append([self.all_queries_index[elem[0]],self.doc_index[elem[1]]])\n",
    "            \n",
    "            self.folds_indexed_qrels.append(indexed_qrels)\n",
    "        \n",
    "        \n",
    "    def compute_info_retrieval(self):\n",
    "        \"\"\"Function that builds the inverted index, the idf of the terms; documents length\n",
    "        and frequencies of terms in the collection and indexes the relations\"\"\"\n",
    "        self.build_inverted_index()\n",
    "        self.compute_idf()\n",
    "        self.compute_docs_length()\n",
    "        self.compute_collection_frequencies()\n",
    "        self.index_relations()\n",
    "        \n",
    "        \n",
    "    def save_results(self,index_queries,results,path,top_k=1000):\n",
    "        with open(path,'w') as f:\n",
    "            for query,documents in enumerate(results):\n",
    "                for i,scores in enumerate(documents.most_common(top_k)):\n",
    "                    f.write(index_queries[query] + ' Q0 ' + self.doc_index[scores[0]] + ' ' + str(i) + ' ' + str(scores[1]) + ' 0\\n')\n",
    "                    \n",
    "                    \n",
    "    def pickle_indexed_collection(self,path):\n",
    "        \"\"\"Function that writes the different indexed collection parts other than documents annd fold queries\n",
    "        into a pickle format\"\"\"\n",
    "        self.documents = None\n",
    "        self.folds_queries = None\n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump(self,f)\n",
    "\n",
    "            \n",
    "    def compute_fasttext_embedding(self,model_path):\n",
    "        \"\"\"Function that computes the embedding matrix using the fasttext embedding: vectors of length \n",
    "        300 for every token in the vocabulary\"\"\"\n",
    "        model = fasttext.load_model(model_path)\n",
    "        dim = model.get_dimension()\n",
    "        vocab_size = int(len(self.vocabulary)/2)\n",
    "        self.embedding_matrix = np.zeros((vocab_size, dim))\n",
    "        for _ in range(vocab_size):\n",
    "            self.embedding_matrix[_] = model[self.vocabulary[_]]\n",
    "                    \n",
    "            \n",
    "    def generate_training_batches(self,fold,batch_size=64):\n",
    "        \"\"\"Function that builds batches of queries and their corresponding negative and positive documents\n",
    "        for training for a particular fold. These batches are picked from outside the fold we want to\n",
    "        test or validate on\"\"\"\n",
    "        positive_pairs = []\n",
    "        negative_pairs = {}\n",
    "        for i in range(self.k):\n",
    "            if i != fold:\n",
    "                positive_pairs += self.folds_training_indexed_qrels[i]['pos']\n",
    "                negative_pairs.update(self.folds_training_indexed_qrels[i]['neg'])\n",
    "        \n",
    "        random.shuffle(positive_pairs)\n",
    "        nb_docs = len(self.indexed_docs)\n",
    "        nb_train_pairs = len(positive_pairs)\n",
    "        query_batches = []\n",
    "        positive_doc_batches = []\n",
    "        negative_doc_batches = []\n",
    "        pos = 0\n",
    "        while(pos + batch_size < nb_train_pairs):\n",
    "            query_batches.append([q for q,d in positive_pairs[pos:pos+batch_size]])\n",
    "            positive_doc_batches.append([d for q,d in positive_pairs[pos:pos+batch_size]])\n",
    "            neg_docs = []\n",
    "            for elem in query_batches[-1]:\n",
    "                neg_docs.append(random.choice(negative_pairs[self.all_queries_index[elem]]))\n",
    "            negative_doc_batches.append(neg_docs)\n",
    "            pos += batch_size\n",
    "        return query_batches,positive_doc_batches,negative_doc_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Trec Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT91-94  finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA  finished\n"
     ]
    }
   ],
   "source": [
    "trec_collections_path=\"/home/mrim/rezguiha/work/repro_chap7_res/TREC/\"\n",
    "fasttext_model_path=\"/home/mrim/rezguiha/work/repro_chap7_res/fastText/cc.en.300.bin\"\n",
    "#Using the next line only if the collection haven't been processed by creating folds containing documents \n",
    "#, queries and qrels for each fold and creating csv files from Xml format\n",
    "# read_collection()\n",
    "\n",
    "for collection in [\"AP88-89\",\"FT91-94\",\"LA\"]:\n",
    "    Collection = TrecCollection(k=5)\n",
    "    Collection.load_collection(trec_collections_path+collection)\n",
    "    Collection.standard_preprocess(remove_stopwords = True,\n",
    "                                   min_occ = 5)\n",
    "    Collection.compute_info_retrieval()\n",
    "    Collection.compute_fasttext_embedding(fasttext_model_path)\n",
    "    Collection.pickle_indexed_collection(trec_collections_path+collection + '/indexed_collection')\n",
    "    print(collection, \" finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building Wikir Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Collection for Wikir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import snowball\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "class Collection:\n",
    "    def __init__(self,language='english'):\n",
    "        self.documents = None\n",
    "        self.training_queries = None\n",
    "        self.validation_queries = None\n",
    "        self.test_queries = None\n",
    "        self.language = language\n",
    "        self.stemmer = snowball.EnglishStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        \n",
    "    def load_collection(self,collection_path):\n",
    "        \"\"\"Function that loads an already processed collection and reads its csv files\"\"\"\n",
    "        self.documents = pd.read_csv(collection_path + '/documents.csv',index_col='id_right',na_filter=False)\n",
    "        self.training_queries = pd.read_csv(collection_path + '/training/queries.csv',index_col='id_left',na_filter=False)\n",
    "        self.validation_queries = pd.read_csv(collection_path + '/validation/queries.csv',index_col='id_left',na_filter=False)\n",
    "        self.test_queries = pd.read_csv(collection_path + '/test/queries.csv',index_col='id_left',na_filter=False)\n",
    "    \n",
    "        self.training_relevance = read_qrels(collection_path + '/training/qrels')\n",
    "        self.validation_relevance = read_qrels(collection_path + '/validation/qrels')\n",
    "        self.test_relevance = read_qrels(collection_path + '/test/qrels')\n",
    "        \n",
    "        \n",
    "    def save_xml(self,output_dir):\n",
    "        \"\"\"Function that saves the documents, the training queries ,validation queries and test queries \n",
    "        generated into xml format\"\"\"\n",
    "        with open(output_dir + '/documents.xml','w') as f:\n",
    "            for key,value in self.documents.iterrows():\n",
    "                f.write('<DOC>\\n<DOCNO>' + str(key) + '</DOCNO>\\n<TEXT>\\n' + value[0] + '\\n</TEXT></DOC>\\n')\n",
    "\n",
    "        with open(output_dir + '/training_queries.xml','w') as f:\n",
    "            for key,value in self.training_queries.iterrows():\n",
    "                f.write('<top>\\n<num>' + str(key) + '</num><title>\\n' + value[0] + '\\n</title>\\n</top>\\n')\n",
    "\n",
    "        with open(output_dir + '/validation_queries.xml','w') as f:\n",
    "            for key,value in self.validation_queries.iterrows():\n",
    "                f.write('<top>\\n<num>' + str(key) + '</num><title>\\n' + value[0] + '\\n</title>\\n</top>\\n')\n",
    "\n",
    "        with open(output_dir + '/test_queries.xml','w') as f:\n",
    "            for key,value in self.test_queries.iterrows():\n",
    "                f.write('<top>\\n<num>' + str(key) + '</num><title>\\n' + value[0] + '\\n</title>\\n</top>\\n')\n",
    "    \n",
    "        \n",
    "    def update_standard_vocabulary(self,sequences,remove_stopwords=True):\n",
    "        \"\"\"Function that updates the vocabulary on the basis of new sequences\"\"\"\n",
    "        count = 0\n",
    "        if remove_stopwords:\n",
    "            for _,sequence in sequences.iterrows():\n",
    "                for word in sequence[0].split(\" \"):\n",
    "                    temp = word.lower()\n",
    "                    if temp not in self.stop_words:\n",
    "                        self.vocabulary[self.stemmer.stem(temp)] += 1\n",
    "                count +=1\n",
    "        else:\n",
    "            for _,sequence in sequences.iterrows():\n",
    "                for word in sequence[0].split(\" \"):\n",
    "                    self.vocabulary[self.stemmer.stem(word.lower())] += 1\n",
    "                count +=1\n",
    "        \n",
    "        \n",
    "    def build_standard_vocabulary(self,\n",
    "                                  min_occ = 2,\n",
    "                                  remove_stopwords = True):\n",
    "        \"\"\"Function that builds the vocabulary from documents and the different queries\"\"\"\n",
    "        self.vocabulary = Counter()\n",
    "    \n",
    "        self.update_standard_vocabulary(self.documents,remove_stopwords)\n",
    "        self.update_standard_vocabulary(self.training_queries,remove_stopwords)\n",
    "        self.update_standard_vocabulary(self.validation_queries,remove_stopwords)\n",
    "        self.update_standard_vocabulary(self.test_queries,remove_stopwords)\n",
    "    \n",
    "        self.vocabulary = {i+1:elem[0] for i,elem in enumerate(self.vocabulary.most_common()) if elem[1] >= min_occ}\n",
    "\n",
    "        for key in list(self.vocabulary):\n",
    "            self.vocabulary[self.vocabulary[key]] = key\n",
    "            \n",
    "        self.vocabulary[0] = '<PAD>'\n",
    "        self.vocabulary['<PAD>'] = 0\n",
    "    \n",
    "     \n",
    "        \n",
    "    def standard_preprocess(self,\n",
    "                            remove_stopwords = True,\n",
    "                            min_occ = 5):\n",
    "        \"\"\"Function that preprocesses the collection by building the vocabulary and indexing the documents \n",
    "        and the different queries\"\"\"\n",
    "        print('Build voc',flush=True)\n",
    "        self.build_standard_vocabulary(min_occ = min_occ,\n",
    "                                       remove_stopwords = remove_stopwords)\n",
    "        \n",
    "        print('Index documents',flush=True)\n",
    "        self.doc_index,self.indexed_docs = std_tokenizer_index(self.documents,\n",
    "                                                             self.vocabulary,\n",
    "                                                             self.stemmer)\n",
    "        \n",
    "        print('Index queries',flush=True)\n",
    "        self.training_queries_index,self.indexed_training_queries = std_tokenizer_index(self.training_queries,\n",
    "                                                                                        self.vocabulary,\n",
    "                                                                                        self.stemmer)\n",
    "        \n",
    "        self.validation_queries_index,self.indexed_validation_queries = std_tokenizer_index(self.validation_queries,\n",
    "                                                                                            self.vocabulary,\n",
    "                                                                                            self.stemmer)\n",
    "        \n",
    "        self.test_queries_index,self.indexed_test_queries = std_tokenizer_index(self.test_queries,\n",
    "                                                                                self.vocabulary,\n",
    "                                                                                self.stemmer)\n",
    "           \n",
    "    \n",
    "    def build_inverted_index(self):\n",
    "        \"\"\"Function that builds the inverted index from the vocabulary and the indexed documents\"\"\"\n",
    "        self.inverted_index = dict()\n",
    "        \n",
    "        for token in self.vocabulary:\n",
    "            if isinstance(token, int):\n",
    "                self.inverted_index[token] = Counter()\n",
    "            \n",
    "        for i,indexed_document in enumerate(self.indexed_docs):\n",
    "            for token in indexed_document:\n",
    "                self.inverted_index[token][i] += np.float32(1.0)\n",
    "                \n",
    "                \n",
    "    def compute_idf(self):\n",
    "        \"\"\"Function that computes the idf for every word\"\"\"\n",
    "        nb_docs = len(self.doc_index)\n",
    "        self.idf = {token:np.log((nb_docs+1)/(1+len(self.inverted_index[token]))) for token in self.inverted_index }\n",
    "        \n",
    "        \n",
    "    def compute_docs_length(self):\n",
    "        \"\"\"Function that computes documents length\"\"\"\n",
    "        self.docs_length = {i:len(doc) for i,doc in enumerate(self.indexed_docs)}\n",
    "        \n",
    "        \n",
    "    def compute_collection_frequencies(self):\n",
    "        \"\"\"Function that computes frequencies of eac word\"\"\"\n",
    "        coll_length = sum([value for key,value in self.docs_length.items()])\n",
    "        self.c_freq = {token:sum([freq for _,freq in self.inverted_index[token].items()])/coll_length for token in self.inverted_index}\n",
    "        \n",
    "    def index_relations(self):\n",
    "        self.training_indexed_relevance = []\n",
    "        for elem in self.training_relevance:\n",
    "            self.training_indexed_relevance.append([self.training_queries_index[elem[0]],self.doc_index[elem[1]]])\n",
    "\n",
    "        self.validation_indexed_relevance = []\n",
    "        for elem in self.validation_relevance:\n",
    "            self.validation_indexed_relevance.append([self.validation_queries_index[elem[0]],self.doc_index[elem[1]]])\n",
    "                \n",
    "        self.test_indexed_relevance = []\n",
    "        for elem in self.test_relevance:\n",
    "            self.test_indexed_relevance.append([self.test_queries_index[elem[0]],self.doc_index[elem[1]]])\n",
    "        \n",
    "        \n",
    "    def compute_info_retrieval(self):\n",
    "        \"\"\"Function that builds inverted index , idf , document length, collection frequencies and indexed relations\"\"\"\n",
    "        self.build_inverted_index()\n",
    "        self.compute_idf()\n",
    "        self.compute_docs_length()\n",
    "        self.compute_collection_frequencies()\n",
    "        self.index_relations()\n",
    "        \n",
    "        \n",
    "    def save_results(self,index_queries,results,path,top_k=1000):\n",
    "        \"\"\"Function that saves the top 1000 results according to their score\"\"\"\n",
    "        with open(path,'w') as f:\n",
    "            for query,documents in enumerate(results):\n",
    "                for i,scores in enumerate(documents.most_common(top_k)):\n",
    "                    f.write(index_queries[query] + ' Q0 ' + self.doc_index[scores[0]] + ' ' + str(i) + ' ' + str(scores[1]) + ' 0\\n')\n",
    "                    \n",
    "                    \n",
    "    def pickle_indexed_collection(self,path):\n",
    "        \"\"\"Function that saves th computed self elements into a pickle file\"\"\"\n",
    "        self.documents = None\n",
    "        self.training_queries = None\n",
    "        self.validation_queries = None\n",
    "        self.test_queries = None\n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump(self,f)\n",
    "\n",
    "            \n",
    "    def compute_fasttext_embedding(self,model_path):\n",
    "        \"\"\"Function that computes the fasttext embedding : vectos of 300 dimension\"\"\"\n",
    "        model = fasttext.load_model(model_path)\n",
    "        dim = model.get_dimension()\n",
    "        vocab_size = int(len(self.vocabulary)/2)\n",
    "        self.embedding_matrix = np.zeros((vocab_size, dim))\n",
    "        for _ in range(vocab_size):\n",
    "            self.embedding_matrix[_] = model[self.vocabulary[_]]\n",
    "                    \n",
    "            \n",
    "    def generate_training_batches(self,batch_size=64):\n",
    "        \"Function that generates training batches\"\n",
    "        random.shuffle(self.training_indexed_relevance)\n",
    "        nb_docs = len(self.indexed_docs)\n",
    "        nb_train_pairs = len(self.training_indexed_relevance)\n",
    "        query_batches = []\n",
    "        positive_doc_batches = []\n",
    "        negative_doc_batches = []\n",
    "        pos = 0\n",
    "        while(pos + batch_size < nb_train_pairs):\n",
    "            query_batches.append([q for q,d in self.training_indexed_relevance[pos:pos+batch_size]])\n",
    "            positive_doc_batches.append([d for q,d in self.training_indexed_relevance[pos:pos+batch_size]])\n",
    "            negative_doc_batches.append([random.randint(0, nb_docs-1) for _ in range(len(positive_doc_batches[-1]))])\n",
    "            pos += batch_size\n",
    "        return query_batches,positive_doc_batches,negative_doc_batches\n",
    "    \n",
    "    \n",
    "    def generate_test_batches(self,batch_size=64):\n",
    "        \"\"\"Function that generates test batch\"\"\"\n",
    "        random.shuffle(self.test_indexed_relevance)\n",
    "        nb_docs = len(self.indexed_docs)\n",
    "        nb_test_pairs = len(self.test_indexed_relevance)\n",
    "        query_batches = []\n",
    "        positive_doc_batches = []\n",
    "        negative_doc_batches = []\n",
    "        pos = 0\n",
    "        while(pos + batch_size < nb_test_pairs):\n",
    "            query_batches.append([q for q,d in self.test_indexed_relevance[pos:pos+batch_size]])\n",
    "            positive_doc_batches.append([d for q,d in self.test_indexed_relevance[pos:pos+batch_size]])\n",
    "            negative_doc_batches.append([random.randint(0, nb_docs-1) for _ in range(len(positive_doc_batches[-1]))])\n",
    "            pos += batch_size\n",
    "        return query_batches,positive_doc_batches,negative_doc_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading collection\n",
      "Standard Preprocess\n",
      "Build voc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-363c95d3efe8>\", line 10, in <module>\n",
      "    Collection_wikir.standard_preprocess(remove_stopwords = True,\n",
      "  File \"<ipython-input-4-e7b98a2d87fb>\", line 99, in standard_preprocess\n",
      "    self.build_standard_vocabulary(min_occ = min_occ,\n",
      "  File \"<ipython-input-4-e7b98a2d87fb>\", line 78, in build_standard_vocabulary\n",
      "    self.update_standard_vocabulary(self.documents,remove_stopwords)\n",
      "  File \"<ipython-input-4-e7b98a2d87fb>\", line 63, in update_standard_vocabulary\n",
      "    self.vocabulary[self.stemmer.stem(temp)] += 1\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/site-packages/nltk/stem/snowball.py\", line 1751, in stem\n",
      "    if word.endswith(suffix):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/inspect.py\", line 705, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/mrim/rezguiha/anaconda3/lib/python3.8/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-363c95d3efe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Standard Preprocess'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m Collection_wikir.standard_preprocess(remove_stopwords = True,\n\u001b[0m\u001b[1;32m     11\u001b[0m                                min_occ = 5)\n",
      "\u001b[0;32m<ipython-input-4-e7b98a2d87fb>\u001b[0m in \u001b[0;36mstandard_preprocess\u001b[0;34m(self, remove_stopwords, min_occ)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Build voc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         self.build_standard_vocabulary(min_occ = min_occ,\n\u001b[0m\u001b[1;32m    100\u001b[0m                                        remove_stopwords = remove_stopwords)\n",
      "\u001b[0;32m<ipython-input-4-e7b98a2d87fb>\u001b[0m in \u001b[0;36mbuild_standard_vocabulary\u001b[0;34m(self, min_occ, remove_stopwords)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_standard_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_standard_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_queries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e7b98a2d87fb>\u001b[0m in \u001b[0;36mupdate_standard_vocabulary\u001b[0;34m(self, sequences, remove_stopwords)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__step4_suffixes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2044\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2047\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2048\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1437\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1194\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "\n",
    "    coll_path=\"/home/mrim/rezguiha/work/repro_chap7_res/wikIR_78\"\n",
    "    fasttext_model_path=\"/home/mrim/rezguiha/work/repro_chap7_res/fastText/cc.en.300.bin\"\n",
    "    index_path=\"/home/mrim/rezguiha/work/repro_chap7_res/enwikIR_indexed\"\n",
    "    print('Reading collection',flush=True)\n",
    "    \n",
    "    Collection_wikir = Collection('english')\n",
    "    Collection_wikir.load_collection(coll_path)\n",
    "    \n",
    "    print('Standard Preprocess',flush=True)\n",
    "    Collection_wikir.standard_preprocess(remove_stopwords = True,\n",
    "                                   min_occ = 5)\n",
    "    \n",
    "    print('Compute inverted index',flush=True)\n",
    "    Collection_wikir.compute_info_retrieval()\n",
    "    \n",
    "    Collection_wikir.compute_fasttext_embedding(fasttext_model_path)\n",
    "    \n",
    "    Collection_wikir.pickle_indexed_collection(index_path+'/indexed_collection' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Training Models on TDV for Trec Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def simple_tf(indexed_queries,inverted_index):\n",
    "    \n",
    "    results = []\n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    result[document] += freq\n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "def weighted_simple_tf(indexed_queries,inverted_index,weights):\n",
    "    \n",
    "    results = []\n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    result[document] += weights[token]*freq\n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "def tf_idf(indexed_queries,inverted_index,idf):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    result[document] += freq*idf[token]\n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "    \n",
    "def dir_language_model(indexed_queries,inverted_index,docs_length,c_freq, mu = 2500):\n",
    "    \n",
    "    results = []\n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    result[document] += np.log(1 + (freq/(mu * c_freq[token]))) + np.log(mu/(docs_length[document] + mu))\n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "    \n",
    "    \n",
    "def Okapi_BM25(indexed_queries,inverted_index,docs_length,idf, k1 = 1.2, b = 0.75):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    avg_docs_len = sum([value for key,value in docs_length.items()])/len(docs_length)\n",
    "    \n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    result[document] += idf[token] * ((k1 + 1)*freq)/(freq + k1*((1-b) + b*docs_length[document]/avg_docs_len))\n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "    \n",
    "def fast_Okapi_BM25(indexed_queries,inverted_index,docs_length,idf,avg_docs_len, k1 = 1.2, b = 0.75):\n",
    "    \n",
    "    results = []\n",
    "        \n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    result[document] += idf[token] * ((k1 + 1)*freq)/(freq + k1*((1-b) + b*docs_length[document]/avg_docs_len))\n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "def weighted_tf_idf(indexed_queries,inverted_index,weights,idf):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    result[document] += weights[token]*freq*idf[token]\n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "    \n",
    "def weighted_dir_language_model(indexed_queries,inverted_index,weights,docs_length,c_freq, mu = 2500):\n",
    "    \n",
    "    results = []\n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    result[document] += weights[token]*(np.log(1 + (freq/(mu * c_freq[token]))) + np.log(mu/(docs_length[document] + mu)))\n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "    \n",
    "    \n",
    "\n",
    "def weighted_Okapi_BM25(indexed_queries,inverted_index,weights,docs_length,idf, k1 = 1.2, b = 0.75):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    avg_docs_len = sum([value for key,value in docs_length.items()])/len(docs_length)\n",
    "    \n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    result[document] += weights[token]*idf[token]*((k1 + 1)*freq)/(freq + k1*((1-b) + b*docs_length[document]/avg_docs_len))\n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Lemur_tf_idf(indexed_queries,inverted_index,docs_length,idf,k1=1.2,b=0.75):\n",
    "    \n",
    "    avg_docs_len = sum([value for key,value in docs_length.items()])/len(docs_length)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    Robertson_tf = k1*freq/(freq + k1 *(1-b + b*docs_length[document]/avg_docs_len))\n",
    "                    result[document] += Robertson_tf*np.power(idf[token],2)\n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def JM_language_model(indexed_queries,inverted_index,docs_length,c_freq, Lambda = 0.15):\n",
    "    results = []\n",
    "    for indexed_query in indexed_queries:\n",
    "        result = Counter()\n",
    "        for token in indexed_query:\n",
    "            if token in inverted_index:\n",
    "                for document,freq in inverted_index[token].items():\n",
    "                    result[document] += np.log(1 + ((1/(c_freq[token]))*(Lambda * freq )/((1-Lambda)*docs_length[document])) )\n",
    "    \n",
    "        if len(result)==0:\n",
    "            result[-1] += 0\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "class diff_simple_TF(Model):\n",
    "    def __init__(self,embedding_matrix,dropout_rate=0.1):\n",
    "        super(simple_TF, self).__init__()\n",
    "        self.vocab_size,self.embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, \n",
    "                                                   self.embedding_dim, \n",
    "                                                   weights=[embedding_matrix],\n",
    "                                                   mask_zero=True)\n",
    "        self.linear = tf.keras.layers.Dense(1,\n",
    "                                            input_shape=(self.embedding_dim,),\n",
    "                                            activation='relu',\n",
    "                                            bias_initializer=tf.ones_initializer())\n",
    "        self.dropout_rate=dropout_rate\n",
    "    \n",
    "    def make_BoW(self,seq,index,sparse_index):\n",
    "        \n",
    "        mask = tf.dtypes.cast(self.embedding.compute_mask(index),dtype=tf.float32)\n",
    "        seq = tf.math.multiply(mask,tf.squeeze(seq))\n",
    "        seq = tf.reshape(seq,[-1])\n",
    "        \n",
    "        seq = tf.SparseTensor(indices = sparse_index, values = seq , dense_shape=[self.vocab_size,index.shape[0]])\n",
    "        seq = tf.sparse.reorder(seq)\n",
    "        \n",
    "        linearized = tf.matmul(seq.indices, tf.constant([[index.shape[0]], [1]],dtype=tf.int64))\n",
    "        y, idx = tf.unique(tf.squeeze(linearized))\n",
    "        values = tf.math.segment_sum(seq.values, idx)\n",
    "        y = tf.expand_dims(y, 1)\n",
    "        indices = tf.concat([y//index.shape[0], y%index.shape[0]], axis=1)\n",
    "        seq = tf.SparseTensor(indices = indices, values = values , dense_shape=[self.vocab_size,index.shape[0]])\n",
    "        seq = tf.sparse.reorder(seq)\n",
    "        \n",
    "        return tf.sparse.to_dense(seq)\n",
    "        \n",
    "    \n",
    "    def call(self, q_index_float_32, q_index, q_sparse_index, d_index, d_sparse_index):\n",
    "        \n",
    "        q = self.make_BoW(q_index_float_32,q_index,q_sparse_index)\n",
    "        \n",
    "        d = tf.nn.dropout(self.embedding(d_index),rate=self.dropout_rate)\n",
    "        d = tf.nn.dropout(self.linear(d),rate=self.dropout_rate)\n",
    "        d = self.make_BoW(d,d_index,d_sparse_index)\n",
    "        \n",
    "        rel = tf.math.reduce_sum(tf.math.multiply(q,d),axis=0)\n",
    "    \n",
    "        return rel,d\n",
    "\n",
    "    def compute_index(self):\n",
    "        index = [_ for _ in range(self.vocab_size)]\n",
    "        \n",
    "        all_embeddings = self.embedding(np.asarray(index))\n",
    "\n",
    "        return np.reshape(self.linear(all_embeddings).numpy(),(self.vocab_size,))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class diff_TF_IDF(Model):\n",
    "    def __init__(self,embedding_matrix,dropout_rate=0.1):\n",
    "        super(TF_IDF, self).__init__()\n",
    "        self.vocab_size,self.embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, \n",
    "                                                   self.embedding_dim, \n",
    "                                                   weights=[embedding_matrix],\n",
    "                                                   mask_zero=True)\n",
    "        self.linear = tf.keras.layers.Dense(1,\n",
    "                                            input_shape=(self.embedding_dim,),\n",
    "                                            activation='relu',\n",
    "                                            bias_initializer=tf.ones_initializer())\n",
    "        self.dropout_rate=dropout_rate\n",
    "    \n",
    "    def make_BoW(self,seq,index,sparse_index):\n",
    "        \n",
    "        mask = tf.dtypes.cast(self.embedding.compute_mask(index),dtype=tf.float32)\n",
    "        seq = tf.math.multiply(mask,tf.squeeze(seq))\n",
    "        seq = tf.reshape(seq,[-1])\n",
    "        \n",
    "        \n",
    "        seq = tf.SparseTensor(indices = sparse_index, values = seq , dense_shape=[self.vocab_size,index.shape[0]])\n",
    "        seq = tf.sparse.reorder(seq)\n",
    "        \n",
    "        linearized = tf.matmul(seq.indices, tf.constant([[index.shape[0]], [1]],dtype=tf.int64))\n",
    "        y, idx = tf.unique(tf.squeeze(linearized))\n",
    "        values = tf.math.segment_sum(seq.values, idx)\n",
    "        y = tf.expand_dims(y, 1)\n",
    "        indices = tf.concat([y//index.shape[0], y%index.shape[0]], axis=1)\n",
    "        seq = tf.SparseTensor(indices = indices, values = values , dense_shape=[self.vocab_size,index.shape[0]])\n",
    "        seq = tf.sparse.reorder(seq)\n",
    "        \n",
    "        return tf.sparse.to_dense(seq)\n",
    "        \n",
    "            \n",
    "    def call(self, q_index_float_32, q_index, q_sparse_index, d_index, d_sparse_index):\n",
    "        \n",
    "        q = self.make_BoW(q_index_float_32,q_index,q_sparse_index)\n",
    "        \n",
    "        d = tf.nn.dropout(self.embedding(d_index),rate=self.dropout_rate)\n",
    "        d = tf.nn.dropout(self.linear(d),rate=self.dropout_rate)\n",
    "        d = self.make_BoW(d,d_index,d_sparse_index)\n",
    "        \n",
    "        maxdf = tf.keras.backend.max(tf.math.reduce_sum(d,axis = 1))\n",
    "        \n",
    "        idf = tf.math.log( (maxdf + 1) / (1+tf.math.reduce_sum(d,axis = 1)))\n",
    "        \n",
    "        idf_d = tf.multiply(d, tf.reshape(idf, (-1, 1)))\n",
    "        \n",
    "        rel = tf.math.reduce_sum(tf.math.multiply(q,idf_d),axis=0)\n",
    "        \n",
    "        return rel,d\n",
    "\n",
    "    def compute_index(self):\n",
    "        index = [_ for _ in range(self.vocab_size)]\n",
    "        \n",
    "        all_embeddings = self.embedding(np.asarray(index))\n",
    "\n",
    "        return np.reshape(self.linear(all_embeddings).numpy(),(self.vocab_size,))\n",
    "    \n",
    "    \n",
    "class diff_DIR(Model):\n",
    "    def __init__(self,embedding_matrix,mu=2500.0,dropout_rate=0.1):\n",
    "        super(DIR, self).__init__()\n",
    "        self.vocab_size,self.embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, \n",
    "                                                   self.embedding_dim, \n",
    "                                                   weights=[embedding_matrix],\n",
    "                                                   mask_zero=True)\n",
    "        self.linear = tf.keras.layers.Dense(1,\n",
    "                                            input_shape=(self.embedding_dim,),\n",
    "                                            activation='relu',\n",
    "                                            bias_initializer=tf.ones_initializer())\n",
    "        self.mu = tf.Variable(mu)\n",
    "        self.dropout_rate=dropout_rate\n",
    "        \n",
    "    def make_BoW(self,seq,index,sparse_index):\n",
    "        \n",
    "        mask = tf.dtypes.cast(self.embedding.compute_mask(index),dtype=tf.float32)\n",
    "        seq = tf.math.multiply(mask,tf.squeeze(seq))\n",
    "        seq = tf.reshape(seq,[-1])\n",
    "        \n",
    "        \n",
    "        seq = tf.SparseTensor(indices = sparse_index, values = seq , dense_shape=[self.vocab_size,index.shape[0]])\n",
    "        seq = tf.sparse.reorder(seq)\n",
    "        \n",
    "        linearized = tf.matmul(seq.indices, tf.constant([[index.shape[0]], [1]],dtype=tf.int64))\n",
    "        y, idx = tf.unique(tf.squeeze(linearized))\n",
    "        values = tf.math.segment_sum(seq.values, idx)\n",
    "        y = tf.expand_dims(y, 1)\n",
    "        indices = tf.concat([y//index.shape[0], y%index.shape[0]], axis=1)\n",
    "        seq = tf.SparseTensor(indices = indices, values = values , dense_shape=[self.vocab_size,index.shape[0]])\n",
    "        seq = tf.sparse.reorder(seq)\n",
    "        \n",
    "        return tf.sparse.to_dense(seq)\n",
    "        \n",
    "            \n",
    "    def call(self, q_index_float_32, q_index, q_sparse_index, d_index, d_sparse_index):\n",
    "        \n",
    "        q = self.make_BoW(q_index_float_32,q_index,q_sparse_index)\n",
    "        \n",
    "        d = tf.nn.dropout(self.embedding(d_index),rate=self.dropout_rate)\n",
    "        d = tf.nn.dropout(self.linear(d),rate=self.dropout_rate)\n",
    "        d = self.make_BoW(d,d_index,d_sparse_index)\n",
    "        \n",
    "        cfreq = tf.math.reduce_sum(d,axis=1)/tf.math.reduce_sum(d)\n",
    "        \n",
    "        smoothing = tf.math.log(self.mu/(tf.math.reduce_sum(d,axis=0) + self.mu))\n",
    "        \n",
    "        dir_d = tf.math.log(1+d/(1+self.mu*tf.reshape(cfreq, (-1, 1)))) + smoothing\n",
    "        \n",
    "        rel = tf.math.reduce_sum(tf.math.multiply(q,dir_d),axis=0)\n",
    "        \n",
    "        return rel,d\n",
    "\n",
    "    def compute_index(self):\n",
    "        index = [_ for _ in range(self.vocab_size)]\n",
    "        \n",
    "        all_embeddings = self.embedding(np.asarray(index))\n",
    "\n",
    "        return np.reshape(self.linear(all_embeddings).numpy(),(self.vocab_size,))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class diff_BM25(Model):\n",
    "    def __init__(self,embedding_matrix,k1=1.2,b=0.75,dropout_rate=0.1):\n",
    "        super(BM25, self).__init__()\n",
    "        self.vocab_size,self.embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, \n",
    "                                                   self.embedding_dim, \n",
    "                                                   weights=[embedding_matrix],\n",
    "                                                   mask_zero=True)\n",
    "        self.linear = tf.keras.layers.Dense(1,\n",
    "                                            input_shape=(self.embedding_dim,),\n",
    "                                            activation='relu',\n",
    "                                            bias_initializer=tf.ones_initializer())\n",
    "        self.k1 = tf.Variable(k1)\n",
    "        self.b = tf.Variable(b)\n",
    "        self.dropout_rate=dropout_rate\n",
    "        \n",
    "    def make_BoW(self,seq,index,sparse_index):\n",
    "        \n",
    "        mask = tf.dtypes.cast(self.embedding.compute_mask(index),dtype=tf.float32)\n",
    "        seq = tf.math.multiply(mask,tf.squeeze(seq))\n",
    "        seq = tf.reshape(seq,[-1])\n",
    "        \n",
    "        \n",
    "        seq = tf.SparseTensor(indices = sparse_index, values = seq , dense_shape=[self.vocab_size,index.shape[0]])\n",
    "        seq = tf.sparse.reorder(seq)\n",
    "        \n",
    "        linearized = tf.matmul(seq.indices, tf.constant([[index.shape[0]], [1]],dtype=tf.int64))\n",
    "        y, idx = tf.unique(tf.squeeze(linearized))\n",
    "        values = tf.math.segment_sum(seq.values, idx)\n",
    "        y = tf.expand_dims(y, 1)\n",
    "        indices = tf.concat([y//index.shape[0], y%index.shape[0]], axis=1)\n",
    "        seq = tf.SparseTensor(indices = indices, values = values , dense_shape=[self.vocab_size,index.shape[0]])\n",
    "        seq = tf.sparse.reorder(seq)\n",
    "        \n",
    "        return tf.sparse.to_dense(seq)\n",
    "        \n",
    "        \n",
    "    def call(self, q_index_float_32, q_index, q_sparse_index, d_index, d_sparse_index):\n",
    "        \n",
    "        q = self.make_BoW(q_index_float_32,q_index,q_sparse_index)\n",
    "        \n",
    "        d = tf.nn.dropout(self.embedding(d_index),rate=self.dropout_rate)\n",
    "        d = tf.nn.dropout(self.linear(d),rate=self.dropout_rate)\n",
    "        d = self.make_BoW(d,d_index,d_sparse_index)\n",
    "        \n",
    "        \n",
    "        maxdf = tf.keras.backend.max(tf.math.reduce_sum(d,axis = 1))\n",
    "        \n",
    "        idf = tf.math.log( (maxdf + 1) / (1+tf.math.reduce_sum(d,axis = 1)))\n",
    "        \n",
    "        d_length = tf.math.reduce_sum(d,axis=0)\n",
    "\n",
    "        avg_d_length = tf.reduce_mean(d_length)\n",
    "                \n",
    "        bm25_d = tf.reshape(idf, (-1, 1))*((self.k1+1)*d)/(d + self.k1*((1-self.b) + self.b*d_length/avg_d_length))\n",
    "        \n",
    "        rel = tf.math.reduce_sum(tf.math.multiply(q,bm25_d),axis=0)\n",
    "        \n",
    "        return rel,d\n",
    "\n",
    "    def compute_index(self):\n",
    "        index = [_ for _ in range(self.vocab_size)]\n",
    "        \n",
    "        all_embeddings = self.embedding(np.asarray(index))\n",
    "\n",
    "        return np.reshape(self.linear(all_embeddings).numpy(),(self.vocab_size,))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions from utils path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(Collection, weights):\n",
    "    \"\"\"Function that updates the inverted index of a collection by erasing the tokens that have 0 as TDV\"\"\"\n",
    "    inverted_index = dict()\n",
    "    for key, value in Collection.inverted_index.items():\n",
    "        if weights[key] == 0:\n",
    "            continue\n",
    "        inverted_index[key] = Counter()\n",
    "        for doc_id in value:\n",
    "            inverted_index[key][doc_id] += weights[key] * Collection.inverted_index[key][doc_id]\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "def compute_idf(Collection, inverted_index, weights=None):\n",
    "    \"\"\"Functions that compute the idf with or without introduction of TDV\"\"\"\n",
    "    nb_docs = len(Collection.doc_index)\n",
    "    if weights is None:\n",
    "        return {token: np.log((nb_docs + 1) / (1 + len(inverted_index[token]))) for token in inverted_index}\n",
    "    else:\n",
    "        sums = {key: sum(inverted_index[key].values()) for key in inverted_index}\n",
    "        maxdf = max(sums.values())\n",
    "        return {token: np.log((maxdf + 1) / (1 + sums[token])) for token in inverted_index}\n",
    "\n",
    "\n",
    "# Here we give the weights when we want the docslengths to be the number of occurence\n",
    "# the weights are here for regularization purposes\n",
    "def compute_docs_length(inverted_index, weights=None):\n",
    "    \"\"\"Function that computes document length with TDV or without it\"\"\"\n",
    "    docs_length = Counter()\n",
    "\n",
    "    if weights is None:\n",
    "        for term, posting in inverted_index.items():\n",
    "            for doc_id, nb_occurence in posting.items():\n",
    "                docs_length[doc_id] += nb_occurence\n",
    "\n",
    "    else:\n",
    "        for term, posting in inverted_index.items():\n",
    "            for doc_id, nb_occurence in posting.items():\n",
    "                docs_length[doc_id] += nb_occurence / weights[term]\n",
    "\n",
    "    return docs_length\n",
    "\n",
    "\n",
    "def compute_collection_frequencies(docs_length, inverted_index):\n",
    "    \"\"\"Function that computes frequency of tokens in a  collection\"\"\"\n",
    "    coll_length = sum([value for key, value in docs_length.items()])\n",
    "    return {token: sum([freq for _, freq in inverted_index[token].items()]) / coll_length for token in inverted_index}\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_inverted_index(inverted_index):\n",
    "    \"\"\"Function that takes an inverted index and calculate its vocabulary size and total number of elements\"\"\"\n",
    "    vocab_size = len(inverted_index)\n",
    "    tot_nb_elem = 0\n",
    "    for key,value in inverted_index.items():\n",
    "        tot_nb_elem += len(value)\n",
    "    return vocab_size,tot_nb_elem\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(coll_path,Collection,queries_index,qrel,results,model_name,save_res=True):\n",
    "    \"\"\"Function that saves the results of retrieval: the top_k documents according to their score for \n",
    "    a certain model identified by model_name. Then, it computes different metrics for IR using the pytrec_eval\n",
    "    package\"\"\"\n",
    "    Collection.save_results(queries_index,results,model_name,top_k=1000)\n",
    "\n",
    "    with open(model_name, 'r') as f_run:\n",
    "        run = pytrec_eval.parse_run(f_run)\n",
    "    if not save_res:\n",
    "        os.remove(model_name)\n",
    "\n",
    "    measures = {\"map\",\"ndcg_cut\",\"recall\",\"P\"}\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrel,measures)\n",
    "\n",
    "    all_metrics = evaluator.evaluate(run)\n",
    "\n",
    "    metrics = {'P_5': 0,\n",
    "     'P_10': 0,\n",
    "     'P_20': 0,\n",
    "     'ndcg_cut_5': 0,\n",
    "     'ndcg_cut_10': 0,\n",
    "     'ndcg_cut_20': 0,\n",
    "     'ndcg_cut_1000': 0,\n",
    "     'map': 0,\n",
    "     'recall_1000': 0}\n",
    "    nb_queries = len(all_metrics)\n",
    "    for key,values in all_metrics.items():\n",
    "        for metric in metrics:\n",
    "            metrics[metric] += values[metric]/nb_queries\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def eval_baseline_index(coll_path,\n",
    "                        Collection,\n",
    "                        fold,\n",
    "                        qrel,\n",
    "                        plot_values,\n",
    "                        results_path,\n",
    "                        model_name,\n",
    "                        epoch):\n",
    "    \"\"\"This function computes the metrics for the baseline models for term matching methods and \n",
    "    updates the plot values dictionary for a certain fold and a certain epoch\"\"\"\n",
    "    print('tf')\n",
    "\n",
    "    results = IR_models.simple_tf(Collection.indexed_queries[fold],\n",
    "                                  Collection.inverted_index)\n",
    "    \n",
    "    if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/tf/'):\n",
    "        os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/tf/')\n",
    "    \n",
    "    metrics = compute_metrics(coll_path,\n",
    "                              Collection,\n",
    "                              Collection.queries_index[fold],\n",
    "                              qrel,\n",
    "                              results,  \n",
    "                              results_path + '/fold' + str(fold) + '/' + model_name + '/tf/' + str(epoch))\n",
    "\n",
    "    plot_values['tf'][0].append(1.0)\n",
    "    plot_values['tf'][1].append(metrics)\n",
    "    \n",
    "    print('tf_idf')\n",
    "\n",
    "    results = tf_idf(Collection.indexed_queries[fold],\n",
    "                               Collection.inverted_index,\n",
    "                               Collection.idf)\n",
    "    \n",
    "    if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/tf_idf/'):\n",
    "        os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/tf_idf/')\n",
    "    \n",
    "    metrics = compute_metrics(coll_path,\n",
    "                              Collection,\n",
    "                              Collection.queries_index[fold],\n",
    "                              qrel,\n",
    "                              results,  \n",
    "                              results_path + '/fold' + str(fold) + '/' + model_name + '/tf_idf/' + str(epoch))\n",
    "\n",
    "    plot_values['tf_idf'][0].append(1.0)\n",
    "    plot_values['tf_idf'][1].append(metrics)\n",
    "\n",
    "    \n",
    "    print('DIR')\n",
    "\n",
    "    results = dir_language_model(Collection.indexed_queries[fold],\n",
    "                                           Collection.inverted_index,\n",
    "                                           Collection.docs_length,\n",
    "                                           Collection.c_freq)\n",
    "    \n",
    "    if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/DIR/'):\n",
    "        os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/DIR/')\n",
    "    \n",
    "    metrics = compute_metrics(coll_path,\n",
    "                              Collection,\n",
    "                              Collection.queries_index[fold],\n",
    "                              qrel,\n",
    "                              results,  \n",
    "                              results_path + '/fold' + str(fold) + '/' + model_name + '/DIR/' + str(epoch))\n",
    "\n",
    "    plot_values['DIR'][0].append(1.0)\n",
    "    plot_values['DIR'][1].append(metrics)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('BM25')\n",
    "\n",
    "    results = Okapi_BM25(Collection.indexed_queries[fold],\n",
    "                                   Collection.inverted_index,\n",
    "                                   Collection.docs_length,\n",
    "                                   Collection.idf)\n",
    "    \n",
    "    if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/BM25/'):\n",
    "        os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/BM25/')\n",
    "    \n",
    "    metrics = compute_metrics(coll_path,\n",
    "                              Collection,\n",
    "                              Collection.queries_index[fold],\n",
    "                              qrel,\n",
    "                              results,  \n",
    "                              results_path + '/fold' + str(fold) + '/' + model_name + '/BM25/' + str(epoch))\n",
    "\n",
    "    plot_values['BM25'][0].append(1.0)\n",
    "    plot_values['BM25'][1].append(metrics)\n",
    "    \n",
    "def utils_compute_info_retrieval(Collection,weights,weighted=True):\n",
    "    \"\"\"Computes inverted index, idf, document length and c_frequency for a collection with TDV weights\"\"\"\n",
    "    inverted_index = build_inverted_index(Collection,weights)\n",
    "    if weighted:\n",
    "        idf = compute_idf(Collection,inverted_index,weights)\n",
    "        docs_length = compute_docs_length(inverted_index)\n",
    "        c_freq = compute_collection_frequencies(docs_length,inverted_index)\n",
    "    else:\n",
    "        idf = compute_idf(Collection,inverted_index)\n",
    "        docs_length = compute_docs_length(inverted_index,weights)\n",
    "        c_freq = compute_collection_frequencies(docs_length,inverted_index)\n",
    "    return inverted_index,idf,docs_length,c_freq\n",
    "\n",
    "\n",
    "def eval_learned_index(coll_path,\n",
    "                       Collection,\n",
    "                       IR_model,\n",
    "                       model,\n",
    "                       qrel,\n",
    "                       plot_values,\n",
    "                       plot_path,\n",
    "                       fold,\n",
    "                       inverted_index,\n",
    "                       weights,\n",
    "                       redefined_idf,\n",
    "                       redefined_docs_length,\n",
    "                       redefined_c_freq,\n",
    "#                        idf,\n",
    "#                        docs_length,\n",
    "#                        c_freq,\n",
    "                       prop_elem_index,\n",
    "                       results_path,\n",
    "                       model_name,\n",
    "                       epoch):\n",
    "    \"\"\"Function\"\"\"\n",
    "    \n",
    "    if IR_model=='tf':\n",
    "    \n",
    "        print('tf')\n",
    "\n",
    "        results = simple_tf(Collection.indexed_queries[fold],\n",
    "                                      inverted_index)\n",
    "\n",
    "        if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/tf/'):\n",
    "            os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/tf/')\n",
    "\n",
    "#         print(results,flush=True)\n",
    "            \n",
    "        metrics = compute_metrics(coll_path,\n",
    "                                  Collection,\n",
    "                                  Collection.queries_index[fold],\n",
    "                                  qrel,\n",
    "                                  results,\n",
    "                                  results_path + '/fold' + str(fold) + '/' + model_name + '/tf/' + str(epoch))\n",
    "\n",
    "        plot_values['tf'][0].append(prop_elem_index)\n",
    "        plot_values['tf'][1].append(metrics)\n",
    "\n",
    "        print('weighted_tf')\n",
    "\n",
    "        results = weighted_simple_tf(Collection.indexed_queries[fold],\n",
    "                                               inverted_index,\n",
    "                                               weights)\n",
    "\n",
    "        if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_tf/'):\n",
    "            os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_tf/')\n",
    "\n",
    "        metrics = compute_metrics(coll_path,\n",
    "                                  Collection,\n",
    "                                  Collection.queries_index[fold],\n",
    "                                  qrel,\n",
    "                                  results,\n",
    "                                  results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_tf/' + str(epoch))\n",
    "\n",
    "        plot_values['weighted_tf'][0].append(prop_elem_index)\n",
    "        plot_values['weighted_tf'][1].append(metrics)\n",
    "\n",
    "    if IR_model=='tf_idf':\n",
    "        \n",
    "        print('tf_idf')\n",
    "\n",
    "        results = tf_idf(Collection.indexed_queries[fold],\n",
    "                                   inverted_index,\n",
    "                                   redefined_idf)\n",
    "\n",
    "        if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/tf_idf/'):\n",
    "            os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/tf_idf/')\n",
    "\n",
    "        metrics = compute_metrics(coll_path,\n",
    "                                  Collection,\n",
    "                                  Collection.queries_index[fold],\n",
    "                                  qrel,\n",
    "                                  results,  \n",
    "                                  results_path + '/fold' + str(fold) + '/' + model_name + '/tf_idf/' + str(epoch))\n",
    "\n",
    "        plot_values['tf_idf'][0].append(prop_elem_index)\n",
    "        plot_values['tf_idf'][1].append(metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('weighted_tf_idf')\n",
    "\n",
    "        results = weighted_tf_idf(Collection.indexed_queries[fold],\n",
    "                                            inverted_index,\n",
    "                                            weights,\n",
    "                                            redefined_idf)\n",
    "\n",
    "        if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_tf_idf/'):\n",
    "            os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_tf_idf/')\n",
    "\n",
    "        metrics = compute_metrics(coll_path,\n",
    "                                  Collection,\n",
    "                                  Collection.queries_index[fold],\n",
    "                                  qrel,\n",
    "                                  results,  \n",
    "                                  results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_tf_idf/' + str(epoch))\n",
    "\n",
    "        plot_values['weighted_tf_idf'][0].append(prop_elem_index)\n",
    "        plot_values['weighted_tf_idf'][1].append(metrics)\n",
    "\n",
    "       \n",
    "    if IR_model=='DIR':\n",
    "        \n",
    "        mu = model.mu.numpy()\n",
    "        \n",
    "        print('DIR')\n",
    "\n",
    "        results = dir_language_model(Collection.indexed_queries[fold],\n",
    "                                               inverted_index,\n",
    "                                               redefined_docs_length,\n",
    "                                               redefined_c_freq,\n",
    "                                               mu=mu)\n",
    "\n",
    "        if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/DIR/'):\n",
    "            os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/DIR/')\n",
    "\n",
    "        metrics = compute_metrics(coll_path,\n",
    "                                  Collection,\n",
    "                                  Collection.queries_index[fold],\n",
    "                                  qrel,\n",
    "                                  results,  \n",
    "                                  results_path + '/fold' + str(fold) + '/' + model_name + '/DIR/' + str(epoch))\n",
    "\n",
    "        plot_values['DIR'][0].append(prop_elem_index)\n",
    "        plot_values['DIR'][1].append(metrics)\n",
    "\n",
    "\n",
    "        print('weighted_DIR')\n",
    "\n",
    "        results = weighted_dir_language_model(Collection.indexed_queries[fold],\n",
    "                                                        inverted_index,\n",
    "                                                        weights,\n",
    "                                                        redefined_docs_length,\n",
    "                                                        redefined_c_freq,\n",
    "                                                        mu=mu)\n",
    "\n",
    "        if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_DIR/'):\n",
    "            os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_DIR/')\n",
    "\n",
    "        metrics = compute_metrics(coll_path,\n",
    "                                  Collection,\n",
    "                                  Collection.queries_index[fold],\n",
    "                                  qrel,\n",
    "                                  results,  \n",
    "                                  results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_DIR/' + str(epoch))\n",
    "\n",
    "        plot_values['weighted_DIR'][0].append(prop_elem_index)\n",
    "        plot_values['weighted_DIR'][1].append(metrics)\n",
    "\n",
    "    \n",
    "    if IR_model=='BM25':\n",
    "        \n",
    "        k1 = model.k1.numpy()\n",
    "        b = model.b.numpy()\n",
    "        \n",
    "        print('BM25')\n",
    "\n",
    "        results = Okapi_BM25(Collection.indexed_queries[fold],\n",
    "                                       inverted_index,\n",
    "                                       redefined_docs_length,\n",
    "                                       redefined_idf,\n",
    "                                       k1=k1,\n",
    "                                       b=b)\n",
    "\n",
    "        if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/BM25/'):\n",
    "            os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/BM25/')\n",
    "\n",
    "        metrics = compute_metrics(coll_path,\n",
    "                                  Collection,\n",
    "                                  Collection.queries_index[fold],\n",
    "                                  qrel,\n",
    "                                  results,  \n",
    "                                  results_path + '/fold' + str(fold) + '/' + model_name + '/BM25/' + str(epoch))\n",
    "\n",
    "        plot_values['BM25'][0].append(prop_elem_index)\n",
    "        plot_values['BM25'][1].append(metrics)\n",
    "\n",
    "\n",
    "\n",
    "        print('weighted_BM25')\n",
    "\n",
    "        results = weighted_Okapi_BM25(Collection.indexed_queries[fold],\n",
    "                                                inverted_index,\n",
    "                                                weights,\n",
    "                                                redefined_docs_length,\n",
    "                                                redefined_idf,\n",
    "                                                k1=k1,\n",
    "                                                b=b)\n",
    "\n",
    "        if not os.path.exists(results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_BM25/'):\n",
    "            os.makedirs( results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_BM25/')\n",
    "\n",
    "        metrics = compute_metrics(coll_path,\n",
    "                                  Collection,\n",
    "                                  Collection.queries_index[fold],\n",
    "                                  qrel,\n",
    "                                  results,  \n",
    "                                  results_path + '/fold' + str(fold) + '/' + model_name + '/weighted_BM25/' + str(epoch))\n",
    "\n",
    "        plot_values['weighted_BM25'][0].append(prop_elem_index)\n",
    "        plot_values['weighted_BM25'][1].append(metrics)\n",
    "\n",
    "    pickle.dump(plot_values,open(plot_path + '/fold' + str(fold) + '/' + model_name,'wb'))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on TREC Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import bpe\n",
    "import time\n",
    "import models\n",
    "import pickle\n",
    "import timeit\n",
    "import argparse\n",
    "import importlib\n",
    "import IR_models\n",
    "import collections\n",
    "import pytrec_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import tensorflow_ranking as tfr\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-c','--coll_path', nargs=\"?\", type=str)\n",
    "    parser.add_argument('-i','--indexed_path', nargs=\"?\", type=str)\n",
    "    parser.add_argument('-p','--plot_path', nargs=\"?\", type=str)\n",
    "    parser.add_argument('-r','--results_path', nargs=\"?\", type=str)\n",
    "    parser.add_argument('-w','--weights_path', nargs=\"?\", type=str)\n",
    "    parser.add_argument('-f','--folds', nargs=\"?\", type=int,default = 5)\n",
    "    parser.add_argument('-e','--nb_epoch', nargs=\"?\", type=int)\n",
    "    parser.add_argument('-l','--l1_weight', nargs=\"?\", type=float)\n",
    "    parser.add_argument('-d','--dropout_rate', nargs=\"?\", type=float,default=0.0)\n",
    "    parser.add_argument('--lr', nargs=\"?\", type=float)\n",
    "    parser.add_argument('-n','--model_name', nargs=\"?\", type=str)\n",
    "    parser.add_argument('--IR_model', nargs=\"?\", type=str,default='tf')\n",
    "    parser.add_argument('-u','--update_embeddings', action=\"store_true\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(args,flush=True)\n",
    "\n",
    "    nb_epoch=100\n",
    "    l1_weight=1e-5\n",
    "    dropout_rate=0.0\n",
    "    folds=5\n",
    "    lr=1e-3\n",
    "    for collection in [\"AP88-89\",\"FT91-94\",\"LA\"]:\n",
    "        coll_path=\"/home/mrim/rezguiha/work/repro_chap7_res/TREC/\"+collection\n",
    "        indexed_path=\"/home/mrim/rezguiha/work/repro_chap7_res/TREC/\"+collection+ \"/indexed_collection\"\n",
    "        results_path=\"/home/mrim/rezguiha/work/repro_chap7_res/TREC/\"+collection+ \"/results\"\n",
    "        weights_path=\"/home/mrim/rezguiha/work/repro_chap7_res/TREC/\"+collection+ \"/weights\"\n",
    "        plot_path=\"/home/mrim/rezguiha/work/repro_chap7_res/TREC/\"+collection+ \"/plots\"\n",
    "        #Loading indexed collection\n",
    "        Collection = TrecCollection()\n",
    "        with open(indexed_path,'rb') as f:\n",
    "            Collection = pickle.load(f)\n",
    "\n",
    "        Collection.doc_index[-1] = \"-1\"\n",
    "        Collection.doc_index[\"-1\"] = -1\n",
    "        #Loading relevance judgements from collection\n",
    "        with open(coll_path + 'qrels' , 'r') as f_qrel:\n",
    "            qrel = pytrec_eval.parse_qrel(f_qrel)\n",
    "        #????\n",
    "        id_titl = Collection.vocabulary['titl']\n",
    "        for i in range(len(Collection.all_indexed_queries)):\n",
    "            if Collection.all_indexed_queries[i][0]==id_titl and len(Collection.all_indexed_queries[i]) > 1:\n",
    "                del Collection.all_indexed_queries[i][0]\n",
    "\n",
    "        for i in range(len(Collection.indexed_queries)):\n",
    "            for j in range(len(Collection.indexed_queries[i])):\n",
    "                if Collection.indexed_queries[i][j][0]==id_titl and len(Collection.indexed_queries[i][j]) > 1:\n",
    "                    del Collection.indexed_queries[i][j][0]\n",
    "\n",
    "        print('start')\n",
    "        #Getting collection vocabulary size and total number of elements in collection\n",
    "        coll_vocab_size,coll_tot_nb_elem = evaluate_inverted_index(Collection.inverted_index)\n",
    "        #Creating for each fold and for every model a directory for results,weights and plots data\n",
    "        \n",
    "        for IR_model in ['tf_idf','DIR','BM25']:\n",
    "            id_model_name= collection+'_'+IR_model+'_'+str(l1_weight)+'_'+str(dropout_rate)\n",
    "\n",
    "            for fold in range(args.folds):\n",
    "\n",
    "                plot_values = dict()\n",
    "\n",
    "                for model_name in ['tf',\n",
    "                                   'tf_idf',\n",
    "                                   'DIR',\n",
    "                                   'BM25']:\n",
    "                    plot_values[model_name] = [[],[]]\n",
    "\n",
    "                if not os.path.exists(results_path + '/fold' + str(fold) + '/' + id_model_name):\n",
    "                    os.makedirs(results_path + '/fold' + str(fold) + '/' + id_model_name)\n",
    "\n",
    "                if not os.path.exists(weights_path + '/fold' + str(fold) + '/' + id_model_name):\n",
    "                    os.makedirs(weights_path + '/fold' + str(fold) + '/' + id_model_name)\n",
    "\n",
    "                if not os.path.exists(plot_path + '/fold' + str(fold) + '/'):\n",
    "                    os.makedirs(plot_path + '/fold' + str(fold) + '/')\n",
    "                #Computing metrics for a certain model and a certain fold and updating plot_values dictionnary\n",
    "                eval_baseline_index(coll_path,\n",
    "                                          Collection,\n",
    "                                          fold,\n",
    "                                          qrel,\n",
    "                                          plot_values,\n",
    "                                          results_path,\n",
    "                                          id_model_name,\n",
    "                                          0)\n",
    "                #Saving plot_values dict of a particular fold as a pickle\n",
    "                pickle.dump(plot_values,open(args.plot_path + '/fold' + str(fold) + '/' + id_model_name,'wb'))\n",
    "                # Initialization of batch size, the loss function,te optimizer and the model to train\n",
    "                batch_gen_time = []\n",
    "                batch_size = 32\n",
    "                y_true = tf.ones(batch_size,)\n",
    "                loss_function = tf.keras.losses.Hinge()\n",
    "                optimizer = tf.keras.optimizers.Adam(args.lr)\n",
    "\n",
    "                if args.IR_model=='tf':\n",
    "                    model = diff_simple_TF(Collection.embedding_matrix,dropout_rate=dropout_rate)\n",
    "\n",
    "                elif args.IR_model=='tf_idf':\n",
    "                    model = diff_TF_IDF(Collection.embedding_matrix,dropout_rate=dropout_rate)\n",
    "\n",
    "                elif args.IR_model=='DIR':\n",
    "                    model = diff_DIR(Collection.embedding_matrix,dropout_rate=dropout_rate)\n",
    "\n",
    "                elif args.IR_model=='BM25':\n",
    "                    model = diff_BM25(Collection.embedding_matrix,dropout_rate=dropout_rate)\n",
    "                #Training the model\n",
    "                epoch = 0\n",
    "                prop_elem_index = 1.0\n",
    "                while epoch < args.nb_epoch and prop_elem_index > 0.05:\n",
    "\n",
    "                    begin = time.time()\n",
    "                    #generation of batches from the trec collection for training\n",
    "                    query_batches,positive_doc_batches,negative_doc_batches = Collection.generate_training_batches(fold,batch_size)\n",
    "\n",
    "                    rank_loss = 0.0\n",
    "                    reg_loss = 0.0\n",
    "                    all_non_zero = 0.0\n",
    "\n",
    "                    begin = time.time()\n",
    "\n",
    "                    for i in range(len(query_batches)):\n",
    "                        with tf.GradientTape() as tape:\n",
    "                            #reshaping queries, pos_documents and neg_documents into a numpy ndarray\n",
    "                            queries = tf.keras.preprocessing.sequence.pad_sequences([Collection.all_indexed_queries[j] for j in query_batches[i]],padding='post')\n",
    "\n",
    "                            pos_documents = tf.keras.preprocessing.sequence.pad_sequences([Collection.indexed_docs[j] for j in positive_doc_batches[i]],padding='post')\n",
    "\n",
    "                            neg_documents = tf.keras.preprocessing.sequence.pad_sequences([Collection.indexed_docs[j] for j in negative_doc_batches[i]],padding='post')\n",
    "                            #Creating sparse querie, pos_document and neg_documents indexes\n",
    "                            q_sparse_index = [[column,j] for j,raw in enumerate(queries) for column in raw]\n",
    "                            pos_d_sparse_index = [[column,j] for j,raw in enumerate(pos_documents) for column in raw]\n",
    "                            neg_d_sparse_index = [[column,j] for j,raw in enumerate(neg_documents) for column in raw]\n",
    "                            #computing relevance and dense document for the negative and positive documents in the batch\n",
    "                            pos_res,pos_d = model(np.clip(queries, 0, 1).astype(np.float32),\n",
    "                                                  queries,\n",
    "                                                  q_sparse_index,\n",
    "                                                  pos_documents,\n",
    "                                                  pos_d_sparse_index)\n",
    "\n",
    "                            neg_res,neg_d = model(np.clip(queries, 0, 1).astype(np.float32),\n",
    "                                                  queries,\n",
    "                                                  q_sparse_index,\n",
    "                                                  neg_documents,\n",
    "                                                  neg_d_sparse_index)\n",
    "                            #Computing the hinge loss and the regularization loss\n",
    "                            ranking_loss = loss_function(y_true=y_true,y_pred=pos_res-neg_res) \n",
    "\n",
    "                            regularization_loss = tf.norm(pos_d+neg_d,ord=1)\n",
    "\n",
    "                            rank_loss += ranking_loss.numpy()\n",
    "                            reg_loss += regularization_loss.numpy()\n",
    "\n",
    "                            all_non_zero += tf.math.count_nonzero(pos_d+neg_d).numpy()\n",
    "\n",
    "                            loss = (1.0-args.l1_weight)*ranking_loss + args.l1_weight*regularization_loss\n",
    "                            #Calculating gradients\n",
    "                            if args.update_embeddings:\n",
    "                                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                            else:\n",
    "                                gradients = tape.gradient(loss, model.trainable_variables[1:])\n",
    "                        #Back propagating the gradients\n",
    "                        if args.update_embeddings:\n",
    "                            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                        else:\n",
    "                            optimizer.apply_gradients(zip(gradients, model.trainable_variables[1:]))\n",
    "\n",
    "                    #Compute the TDVs after the training and saving them \n",
    "                    weights = model.compute_index()\n",
    "\n",
    "                    pickle.dump(weights,open(args.weights_path + '/fold' + str(fold) + '/' + id_model_name + '/epoch_' + str(epoch),'wb'))\n",
    "\n",
    "                    inverted_index,redefined_idf,redefined_docs_length,redefined_c_freq = utils_compute_info_retrieval(Collection,\n",
    "                                                                                                                       weights,\n",
    "                                                                                                                       weighted=True)\n",
    "\n",
    "            #             inverted_index,idf,docs_length,c_freq = utils.compute_info_retrieval(Collection,\n",
    "            #                                                                                  weights,\n",
    "            #                                                                                  weighted=False)\n",
    "                    #Computing new vocab_size and total number of elements after introducting the TDV\n",
    "                    vocab_size,tot_nb_elem = evaluate_inverted_index(inverted_index)\n",
    "\n",
    "                    print(str(100*vocab_size/coll_vocab_size)[0:5] + '% of the vocabulary is kept')\n",
    "                    print(str(100*tot_nb_elem/coll_tot_nb_elem)[0:5] + '% of the index is kept',flush=True)\n",
    "\n",
    "                    prop_elem_index = tot_nb_elem/coll_tot_nb_elem\n",
    "\n",
    "                    utils.eval_learned_index(coll_path,\n",
    "                                               Collection,\n",
    "                                               IR_model,\n",
    "                                               model,\n",
    "                                               qrel,\n",
    "                                               plot_values,\n",
    "                                               plot_path,\n",
    "                                               fold,\n",
    "                                               inverted_index,\n",
    "                                               weights,\n",
    "                                               redefined_idf,\n",
    "                                               redefined_docs_length,\n",
    "                                               redefined_c_freq,\n",
    "            #                                        idf,\n",
    "            #                                        docs_length,\n",
    "            #                                        c_freq,\n",
    "                                               prop_elem_index,\n",
    "                                               results_path,\n",
    "                                               id_model_name,\n",
    "                                               epoch+1)\n",
    "                    epoch += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
